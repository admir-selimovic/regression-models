{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1JtHVt7Z1do"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits import mplot3d\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression with various optimization techniques: Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent. Demonstrates how to create a simple linear regression model, define loss functions, and train the model using different optimization methods."
      ],
      "metadata": {
        "id": "IV8JckR8Z8VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class plot_error_surfaces\n",
        "class plot_error_surfaces(object):\n",
        "    \"\"\"\n",
        "    This class is designed to visualize the data space and parameter space during training.\n",
        "\n",
        "    Args:\n",
        "        w_range (float): Range for the parameter w.\n",
        "        b_range (float): Range for the parameter b.\n",
        "        X (torch.Tensor): Input data.\n",
        "        Y (torch.Tensor): Target data.\n",
        "        n_samples (int, optional): Number of samples for creating surfaces. Default is 30.\n",
        "        go (bool, optional): Whether to generate plots during initialization. Default is True.\n",
        "    \"\"\"\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self, w_range, b_range, X, Y, n_samples=30, go=True):\n",
        "        \"\"\"\n",
        "        Initializes the plot_error_surfaces object.\n",
        "\n",
        "        Args:\n",
        "            w_range (float): Range for the parameter w.\n",
        "            b_range (float): Range for the parameter b.\n",
        "            X (torch.Tensor): Input data.\n",
        "            Y (torch.Tensor): Target data.\n",
        "            n_samples (int, optional): Number of samples for creating surfaces. Default is 30.\n",
        "            go (bool, optional): Whether to generate plots during initialization. Default is True.\n",
        "        \"\"\"\n",
        "        # Create ranges for w and b\n",
        "        W = np.linspace(-w_range, w_range, n_samples)\n",
        "        B = np.linspace(-b_range, b_range, n_samples)\n",
        "        w, b = np.meshgrid(W, B)\n",
        "\n",
        "        # Initialize the loss surface Z\n",
        "        Z = np.zeros((30, 30))\n",
        "        count1 = 0\n",
        "        self.y = Y.detach().numpy()  # Convert Y to NumPy array\n",
        "        self.x = X.detach().numpy()  # Convert X to NumPy array\n",
        "\n",
        "        # Calculate the loss surface\n",
        "        for w1, b1 in zip(w, b):\n",
        "            count2 = 0\n",
        "            for w2, b2 in zip(w1, b1):\n",
        "                Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2)\n",
        "                count2 += 1\n",
        "            count1 += 1\n",
        "\n",
        "        # Store variables for visualization\n",
        "        self.Z = Z\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "        self.W = []\n",
        "        self.B = []\n",
        "        self.LOSS = []\n",
        "        self.n = 0\n",
        "\n",
        "        # Generate 3D and contour plots if 'go' is True\n",
        "        if go:\n",
        "            plt.figure(figsize=(7.5, 5))\n",
        "            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
        "            plt.title('Loss Surface')\n",
        "            plt.xlabel('w')\n",
        "            plt.ylabel('b')\n",
        "            plt.show()\n",
        "            plt.figure()\n",
        "            plt.title('Loss Surface Contour')\n",
        "            plt.xlabel('w')\n",
        "            plt.ylabel('b')\n",
        "            plt.contour(self.w, self.b, self.Z)\n",
        "            plt.show()\n",
        "\n",
        "    # Setter\n",
        "    def set_para_loss(self, W, B, loss):\n",
        "        \"\"\"\n",
        "        Store parameter values and loss for plotting purposes.\n",
        "\n",
        "        Args:\n",
        "            W (float): Value of parameter w.\n",
        "            B (float): Value of parameter b.\n",
        "            loss (float): Loss value.\n",
        "        \"\"\"\n",
        "        self.n = self.n + 1\n",
        "        self.W.append(W)\n",
        "        self.B.append(B)\n",
        "        self.LOSS.append(loss)\n",
        "\n",
        "    # Plot diagram\n",
        "    def final_plot(self):\n",
        "        \"\"\"\n",
        "        Plot the final diagram with the loss surface and parameter updates.\n",
        "        \"\"\"\n",
        "        ax = plt.axes(projection='3d')\n",
        "        ax.plot_wireframe(self.w, self.b, self.Z)\n",
        "        ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)\n",
        "        plt.figure()\n",
        "        plt.contour(self.w, self.b, self.Z)\n",
        "        plt.scatter(self.W, self.B, c='r', marker='x')\n",
        "        plt.xlabel('w')\n",
        "        plt.ylabel('b')\n",
        "        plt.show()\n",
        "\n",
        "    # Plot diagram\n",
        "    def plot_ps(self):\n",
        "        \"\"\"\n",
        "        Plot the data space and loss surface contour for each iteration.\n",
        "        \"\"\"\n",
        "        plt.subplot(121)\n",
        "        plt.ylim\n",
        "        plt.plot(self.x, self.y, 'ro', label=\"training points\")\n",
        "        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\")\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.ylim((-10, 15))\n",
        "        plt.title('Data Space Iteration: ' + str(self.n))\n",
        "        plt.subplot(122)\n",
        "        plt.contour(self.w, self.b, self.Z)\n",
        "        plt.scatter(self.W, self.B, c='r', marker='x')\n",
        "        plt.title('Loss Surface Contour Iteration' + str(self.n))\n",
        "        plt.xlabel('w')\n",
        "        plt.ylabel('b')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "TKRg-kugZ6Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data\n",
        "torch.manual_seed(1)\n",
        "X = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
        "f = 1 * X - 1\n",
        "Y = f + 0.1 * torch.randn(X.size())\n",
        "plt.plot(X.numpy(), Y.numpy(), 'rx', label='y')\n",
        "plt.plot(X.numpy(), f.numpy(), label='f')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J-WjxSUZbcSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the forward function and MSE Loss function\n",
        "def forward(x):\n",
        "    return w * x + b\n",
        "\n",
        "def criterion(yhat, y):\n",
        "    return torch.mean((yhat - y) ** 2)"
      ],
      "metadata": {
        "id": "qdZq2EhfcvD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create plot_error_surfaces for viewing the data\n",
        "get_surface = plot_error_surfaces(15, 13, X, Y, 30)"
      ],
      "metadata": {
        "id": "mwJlwf-fcnuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Gradient Descent"
      ],
      "metadata": {
        "id": "lPHHtXkadxDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for training the model using Batch Gradient Descent\n",
        "def train_model(iter):\n",
        "    \"\"\"\n",
        "    Train the model using Batch Gradient Descent optimization technique.\n",
        "\n",
        "    Args:\n",
        "        iter (int): Number of iterations for training.\n",
        "    \"\"\"\n",
        "    # Loop through the specified number of epochs\n",
        "    for epoch in range(iter):\n",
        "\n",
        "        # Make a prediction using the current parameters\n",
        "        Yhat = forward(X)\n",
        "\n",
        "        # Calculate the loss using the mean squared error (MSE) criterion\n",
        "        loss = criterion(Yhat, Y)\n",
        "\n",
        "        # Record the current parameter values and loss for visualization\n",
        "        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n",
        "        get_surface.plot_ps()\n",
        "\n",
        "        # Store the loss value in the list LOSS_BGD\n",
        "        LOSS_BGD.append(loss.detach().numpy())\n",
        "\n",
        "        # Perform backward pass to compute gradients of the loss with respect to parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters w and b using the learning rate and gradients\n",
        "        w.data = w.data - lr * w.grad.data\n",
        "        b.data = b.data - lr * b.grad.data\n",
        "\n",
        "        # Zero out the gradients to prepare for the next iteration\n",
        "        w.grad.data.zero_()\n",
        "        b.grad.data.zero_()\n"
      ],
      "metadata": {
        "id": "HUzqX2nmcyPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(-15.0, requires_grad=True)\n",
        "b = torch.tensor(-15.0, requires_grad=True)\n",
        "lr = 0.1\n",
        "LOSS_BGD = []"
      ],
      "metadata": {
        "id": "DLIAAhtIdrgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with n iterations\n",
        "train_model(10)"
      ],
      "metadata": {
        "id": "Yt-l4cMoedy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "XLtePTMfeCdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_SGD(iter):\n",
        "    \"\"\"\n",
        "    Train the model using Stochastic Gradient Descent (SGD) optimization technique.\n",
        "\n",
        "    Args:\n",
        "        iter (int): Number of iterations for training.\n",
        "    \"\"\"\n",
        "    # Loop through the specified number of epochs\n",
        "    for epoch in range(iter):\n",
        "\n",
        "        # SGD is an approximation of the true total loss/cost\n",
        "        # Calculate the true loss for the current model parameters\n",
        "        Yhat = forward(X)\n",
        "        true_loss = criterion(Yhat, Y)\n",
        "\n",
        "        # Store the true loss value in the list LOSS_SGD\n",
        "        LOSS_SGD.append(criterion(Yhat, Y).detach().numpy())\n",
        "\n",
        "        # Iterate through the data points for each epoch (Stochastic part)\n",
        "        for x, y in zip(X, Y):\n",
        "\n",
        "            # Make a prediction using the current parameters\n",
        "            yhat = forward(x)\n",
        "\n",
        "            # Calculate the loss for the current data point\n",
        "            loss = criterion(yhat, y)\n",
        "\n",
        "            # Record the current parameter values and loss for visualization\n",
        "            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n",
        "\n",
        "            # Perform backward pass to compute gradients of the loss with respect to parameters\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters w and b using the learning rate and gradients\n",
        "            w.data = w.data - lr * w.grad.data\n",
        "            b.data = b.data - lr * b.grad.data\n",
        "\n",
        "            # Zero out the gradients to prepare for the next iteration\n",
        "            w.grad.data.zero_()\n",
        "            b.grad.data.zero_()\n",
        "\n",
        "        # Plot the surface and data space after each epoch\n",
        "        get_surface.plot_ps()"
      ],
      "metadata": {
        "id": "SGMugUetfJ57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_surface = plot_error_surfaces(15, 13, X, Y, 30, go=False)\n",
        "LOSS_SGD = []\n",
        "w = torch.tensor(-15.0, requires_grad=True)\n",
        "b = torch.tensor(-10.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "lYviLg28d3qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with n iterations\n",
        "train_model_SGD(10)"
      ],
      "metadata": {
        "id": "nE8izcVqeaY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot LOSS_BGD and LOSS_SGD\n",
        "plt.plot(LOSS_BGD, label=\"Batch Gradient Descent\")\n",
        "plt.plot(LOSS_SGD, label=\"Stochastic Gradient Descent\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Cost/total loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xy-9jwzGffPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGD with Dataset DataLoader"
      ],
      "metadata": {
        "id": "_B3A0_dAfj3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom Dataset class\n",
        "class Data(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for generating synthetic data.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor for the Data class.\n",
        "        Initializes the dataset with synthetic data points.\n",
        "        \"\"\"\n",
        "        # Generate x values within the range [-3, 3] with a step of 0.1\n",
        "        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
        "\n",
        "        # Calculate corresponding y values using a linear equation y = 1 * x - 1\n",
        "        self.y = 1 * self.x - 1\n",
        "\n",
        "        # Store the number of data points in the dataset\n",
        "        self.len = self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Getter method to retrieve a specific data point.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of the desired data point.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the x and y values of the data point.\n",
        "        \"\"\"\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the total number of data points in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of data points in the dataset.\n",
        "        \"\"\"\n",
        "        return self.len\n"
      ],
      "metadata": {
        "id": "i0VKmuWwfnnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and DataLoader\n",
        "dataset = Data()\n",
        "get_surface = plot_error_surfaces(15, 13, X, Y, 30, go=False)\n",
        "trainloader = DataLoader(dataset=dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "76BpmbgvfhZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model using DataLoader\n",
        "w = torch.tensor(-15.0, requires_grad=True)\n",
        "b = torch.tensor(-10.0, requires_grad=True)\n",
        "LOSS_Loader = []"
      ],
      "metadata": {
        "id": "7HiEeYmKfo9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model using Stochastic Gradient Descent (SGD) with DataLoader\n",
        "def train_model_DataLoader(epochs):\n",
        "    \"\"\"\n",
        "    Train the model using Stochastic Gradient Descent (SGD) with DataLoader.\n",
        "\n",
        "    Args:\n",
        "        epochs (int): The number of epochs for training.\n",
        "\n",
        "    This function trains the model using Stochastic Gradient Descent (SGD) with DataLoader,\n",
        "    iterating over the dataset in batches for a specified number of epochs.\n",
        "\n",
        "    During each epoch:\n",
        "    - A prediction Yhat is made using the forward pass.\n",
        "    - The true loss is computed using the criterion.\n",
        "    - The loss is stored in the LOSS_Loader list.\n",
        "    - For each batch of data (x, y) from the DataLoader:\n",
        "        - A prediction yhat is made using the forward function.\n",
        "        - The loss is calculated using the criterion.\n",
        "        - The plotting parameters are updated using set_para_loss.\n",
        "        - The backward pass computes gradients of the loss with respect to parameters.\n",
        "        - The parameters w and b are updated using the gradients and learning rate.\n",
        "        - Gradients are cleared.\n",
        "    - The surface and data space are plotted using get_surface.plot_ps().\n",
        "\n",
        "    This process is repeated for the specified number of epochs.\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # SGD is an approximation of the true total loss/cost\n",
        "        # Compute the true loss using the forward pass\n",
        "        Yhat = forward(X)\n",
        "\n",
        "        # Store the loss in LOSS_Loader\n",
        "        LOSS_Loader.append(criterion(Yhat, Y).detach().numpy())\n",
        "\n",
        "        # Iterate over each batch of data in the DataLoader\n",
        "        for x, y in trainloader:\n",
        "\n",
        "            # Make a prediction using the forward function\n",
        "            yhat = forward(x)\n",
        "\n",
        "            # Calculate the loss using the criterion\n",
        "            loss = criterion(yhat, y)\n",
        "\n",
        "            # Update plotting parameters using set_para_loss\n",
        "            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n",
        "\n",
        "            # Backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters w and b using gradients and learning rate\n",
        "            w.data = w.data - lr * w.grad.data\n",
        "            b.data = b.data - lr * b.grad.data\n",
        "\n",
        "            # Clear gradients\n",
        "            w.grad.data.zero_()\n",
        "            b.grad.data.zero_()\n",
        "\n",
        "        # Plot surface and data space after each epoch\n",
        "        get_surface.plot_ps()\n"
      ],
      "metadata": {
        "id": "mFzn_v3afppw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with n iterations\n",
        "train_model_DataLoader(10)"
      ],
      "metadata": {
        "id": "DpIwx2qwfvLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot LOSS_BGD and LOSS_Loader\n",
        "plt.plot(LOSS_BGD, label=\"Batch Gradient Descent\")\n",
        "plt.plot(LOSS_Loader, label=\"Stochastic Gradient Descent with DataLoader\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Cost/total loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QyYtJs8afy5d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
